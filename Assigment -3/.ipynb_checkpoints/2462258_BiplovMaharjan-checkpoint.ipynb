{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Final Portfolio Project 2026\n",
                "## 5CS037 - Concepts and Technologies of AI\n",
                "\n",
                "**Student Name:** Biplov Maharjan  \n",
                "**Student ID:** 2462258  \n",
                "\n",
                "---\n",
                "\n",
                "This notebook contains the complete implementation for the Final Portfolio Project, covering both Classification and Regression tasks.\n",
                "\n",
                "## Table of Contents\n",
                "1. [Setup and Initialization](#setup)\n",
                "2. [Classification Task](#classification)\n",
                "3. [Regression Task](#regression)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Initialization <a id='setup'></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
                "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
                "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
                "from sklearn.svm import SVC, SVR\n",
                "from sklearn.feature_selection import RFE\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "\n",
                "# Set plot style\n",
                "sns.set(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 8)\n",
                "\n",
                "print(\"Libraries imported successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Load Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define paths\n",
                "class_data_path = r'classification/student_depression_dataset.csv'\n",
                "reg_data_path = r'regression/avocado.csv'\n",
                "\n",
                "# Load data\n",
                "try:\n",
                "    df_class = pd.read_csv(class_data_path)\n",
                "    print(f\"Classification Dataset Loaded: {df_class.shape}\")\n",
                "    display(df_class.head())\n",
                "except FileNotFoundError:\n",
                "    print(\"Classification dataset not found. Please check the path.\")\n",
                "\n",
                "try:\n",
                "    df_reg = pd.read_csv(reg_data_path)\n",
                "    # Check if first column is unnamed index\n",
                "    if 'Unnamed: 0' in df_reg.columns:\n",
                "        df_reg = df_reg.drop('Unnamed: 0', axis=1)\n",
                "    print(f\"Regression Dataset Loaded: {df_reg.shape}\")\n",
                "    display(df_reg.head())\n",
                "except FileNotFoundError:\n",
                "    print(\"Regression dataset not found. Please check the path.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Classification Task <a id='classification'></a>\n",
                "\n",
                "**Goal:** Predict whether a student is suffering from depression based on various academic and lifestyle factors.\n",
                "**Target Variable:** `Depression` (0 or 1)\n",
                "**UNSDG Alignment:** Goal 3 - Good Health and Well-being.\n",
                "\n",
                "### 2.1 Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values and data types\n",
                "print(df_class.info())\n",
                "print(\"\\nMissing Values:\\n\", df_class.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Target Distribution\n",
                "plt.figure(figsize=(6, 4))\n",
                "sns.countplot(x='Depression', data=df_class)\n",
                "plt.title('Distribution of Depression (Target Variable)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Age Distribution\n",
                "plt.figure(figsize=(8, 5))\n",
                "sns.histplot(df_class['Age'], bins=20, kde=True)\n",
                "plt.title('Age Distribution of Students')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Relationship between Sleep Duration and Depression\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.countplot(x='Sleep Duration', hue='Depression', data=df_class)\n",
                "plt.title('Sleep Duration vs Depression')\n",
                "plt.xticks(rotation=45)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Academic Pressure vs Depression\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.boxplot(x='Depression', y='Academic Pressure', data=df_class)\n",
                "plt.title('Academic Pressure vs Depression')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Drop irrelevant or ID columns\n",
                "# 'id' is unique identifier, 'City' might be high cardinality (we can keep if useful, but dropping for simplicity)\n",
                "df_class_clean = df_class.drop(columns=['id', 'City'], errors='ignore')\n",
                "\n",
                "# Handle Categorical Variables\n",
                "# Ordinal Encoding for 'Sleep Duration' and 'Dietary Habits'\n",
                "sleep_map = {'Less than 5 hours': 0, '5-6 hours': 1, '7-8 hours': 2, 'More than 8 hours': 3, 'Others': 1}\n",
                "diet_map = {'Unhealthy': 0, 'Moderate': 1, 'Healthy': 2, 'Others': 1}\n",
                "\n",
                "df_class_clean['Sleep Duration'] = df_class_clean['Sleep Duration'].map(sleep_map).fillna(1)\n",
                "df_class_clean['Dietary Habits'] = df_class_clean['Dietary Habits'].map(diet_map).fillna(1)\n",
                "\n",
                "# Label Encoding for other categoricals\n",
                "le = LabelEncoder()\n",
                "cat_cols = df_class_clean.select_dtypes(include=['object']).columns\n",
                "\n",
                "for col in cat_cols:\n",
                "    df_class_clean[col] = le.fit_transform(df_class_clean[col].astype(str))\n",
                "\n",
                "print(\"Data Encoded Successfully\")\n",
                "display(df_class_clean.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation Matrix\n",
                "plt.figure(figsize=(14, 10))\n",
                "sns.heatmap(df_class_clean.corr(), annot=True, fmt='.2f', cmap='coolwarm')\n",
                "plt.title('Correlation Feature Map')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split Data\n",
                "X = df_class_clean.drop('Depression', axis=1)\n",
                "y = df_class_clean['Depression']\n",
                "\n",
                "# Scale Features\n",
                "scaler = StandardScaler()\n",
                "X_scaled = scaler.fit_transform(X)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "print(\"Training Shape:\", X_train.shape)\n",
                "print(\"Testing Shape:\", X_test.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 Task 1: Neural Network Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build Model\n",
                "model_nn = Sequential([\n",
                "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
                "    Dropout(0.2),\n",
                "    Dense(32, activation='relu'),\n",
                "    Dropout(0.2),\n",
                "    Dense(1, activation='sigmoid')\n",
                "])\n",
                "\n",
                "# Compile\n",
                "model_nn.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
                "\n",
                "# Train\n",
                "history = model_nn.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
                "\n",
                "# Evaluate\n",
                "loss, acc = model_nn.evaluate(X_test, y_test)\n",
                "print(f\"\\nNeural Network Test Accuracy: {acc:.4f}\")\n",
                "\n",
                "# Predictions for Metrics\n",
                "y_pred_nn = (model_nn.predict(X_test) > 0.5).astype(\"int32\")\n",
                "print(\"\\nClassification Report (Neural Network):\")\n",
                "print(classification_report(y_test, y_pred_nn))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot Training History\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
                "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
                "plt.title('Neural Network Training History')\n",
                "plt.xlabel('Epochs')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.4 Task 2: Classical Machine Learning Models\n",
                "We will use **Logistic Regression** and **Random Forest Classifier** as our two primary models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Logistic Regression\n",
                "log_reg = LogisticRegression(random_state=42)\n",
                "log_reg.fit(X_train, y_train)\n",
                "y_pred_log = log_reg.predict(X_test)\n",
                "\n",
                "print(\"Logistic Regression Performance:\")\n",
                "print(f\"Accuracy: {accuracy_score(y_test, y_pred_log):.4f}\")\n",
                "print(classification_report(y_test, y_pred_log))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Random Forest Classifier\n",
                "rf_clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
                "rf_clf.fit(X_train, y_train)\n",
                "y_pred_rf = rf_clf.predict(X_test)\n",
                "\n",
                "print(\"Random Forest Performance:\")\n",
                "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
                "print(classification_report(y_test, y_pred_rf))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.5 Hyperparameter Optimization\n",
                "We will perform hyperparameter tuning on the Random Forest model as it generally performs better."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define Hyperparameter Grid for Random Forest\n",
                "param_grid = {\n",
                "    'n_estimators': [50, 100, 200],\n",
                "    'max_depth': [None, 10, 20, 30],\n",
                "    'min_samples_split': [2, 5, 10]\n",
                "}\n",
                "\n",
                "# Randomized Search (faster than Grid Search)\n",
                "rf_random = RandomizedSearchCV(estimator=RandomForestClassifier(random_state=42), \n",
                "                               param_distributions=param_grid, \n",
                "                               n_iter=10, \n",
                "                               cv=3, \n",
                "                               verbose=1, \n",
                "                               random_state=42, \n",
                "                               n_jobs=-1)\n",
                "\n",
                "rf_random.fit(X_train, y_train)\n",
                "\n",
                "print(\"Best Parameters:\", rf_random.best_params_)\n",
                "best_rf = rf_random.best_estimator_\n",
                "\n",
                "# Evaluate Best Model\n",
                "y_pred_best_rf = best_rf.predict(X_test)\n",
                "print(\"\\nOptimized Random Forest Accuracy:\", accuracy_score(y_test, y_pred_best_rf))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.6 Feature Selection\n",
                "Using Recursive Feature Elimination (RFE) to select the most important features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rfe = RFE(estimator=RandomForestClassifier(random_state=42, n_estimators=50), n_features_to_select=10)\n",
                "rfe.fit(X_train, y_train)\n",
                "\n",
                "selected_features = pd.DataFrame({'Feature': X.columns, 'Selected': rfe.support_})\n",
                "print(\"Top 10 Selected Features:\")\n",
                "print(selected_features[selected_features['Selected'] == True])\n",
                "\n",
                "# Retrain with Selected Features\n",
                "X_train_rfe = rfe.transform(X_train)\n",
                "X_test_rfe = rfe.transform(X_test)\n",
                "\n",
                "rf_rfe = RandomForestClassifier(random_state=42, n_estimators=100)\n",
                "rf_rfe.fit(X_train_rfe, y_train)\n",
                "y_pred_rfe = rf_rfe.predict(X_test_rfe)\n",
                "\n",
                "print(\"\\nRandom Forest with 10 Selected Features Accuracy:\", accuracy_score(y_test, y_pred_rfe))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Regression Task <a id='regression'></a>\n",
                "\n",
                "**Goal:** Predict the average price of avocados based on features like volume, bag type, and region.\n",
                "**Target Variable:** `AveragePrice` (Continuous)\n",
                "**UNSDG Alignment:** Goal 12 - Responsible Consumption and Production."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.1 Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check info\n",
                "print(df_reg.info())\n",
                "print(\"\\nMissing Values:\\n\", df_reg.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution of Target (Average Price)\n",
                "plt.figure(figsize=(8, 5))\n",
                "sns.histplot(df_reg['AveragePrice'], bins=30, kde=True, color='green')\n",
                "plt.title('Distribution of Avocado Prices')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Price over Time\n",
                "df_reg['Date'] = pd.to_datetime(df_reg['Date'])\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.lineplot(x='Date', y='AveragePrice', data=df_reg, hue='type')\n",
                "plt.title('Avocado Price Trend Over Time')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Price vs Type\n",
                "plt.figure(figsize=(8, 5))\n",
                "sns.boxplot(x='type', y='AveragePrice', data=df_reg)\n",
                "plt.title('Price Comparison by Type')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract features from Date\n",
                "df_reg['Month'] = df_reg['Date'].dt.month\n",
                "\n",
                "# Drop Date and ID-like columns (Unnamed already dropped)\n",
                "df_reg_clean = df_reg.drop(columns=['Date'], errors='ignore')\n",
                "\n",
                "# Label Encode Categorical Variables (type, region)\n",
                "le_reg = LabelEncoder()\n",
                "for col in ['type', 'region']:\n",
                "    df_reg_clean[col] = le_reg.fit_transform(df_reg_clean[col])\n",
                "\n",
                "# Correlation Matrix\n",
                "plt.figure(figsize=(12, 8))\n",
                "sns.heatmap(df_reg_clean.corr(), annot=True, fmt='.2f', cmap='Greens')\n",
                "plt.title('Regression Feature Correlation')\n",
                "plt.show()\n",
                "\n",
                "# Split Data\n",
                "X_reg = df_reg_clean.drop('AveragePrice', axis=1)\n",
                "y_reg = df_reg_clean['AveragePrice']\n",
                "\n",
                "# Scale Features\n",
                "scaler_reg = StandardScaler()\n",
                "X_reg_scaled = scaler_reg.fit_transform(X_reg)\n",
                "\n",
                "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg_scaled, y_reg, test_size=0.2, random_state=42)\n",
                "\n",
                "print(\"Regression Train Shape:\", X_train_reg.shape)\n",
                "print(\"Regression Test Shape:\", X_test_reg.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Task 1: Neural Network Regression Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build Model\n",
                "model_nn_reg = Sequential([\n",
                "    Dense(128, activation='relu', input_shape=(X_train_reg.shape[1],)),\n",
                "    Dropout(0.2),\n",
                "    Dense(64, activation='relu'),\n",
                "    Dense(32, activation='relu'),\n",
                "    Dense(1)  # Linear activation for regression\n",
                "])\n",
                "\n",
                "# Compile\n",
                "model_nn_reg.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
                "\n",
                "# Train\n",
                "history_reg = model_nn_reg.fit(X_train_reg, y_train_reg, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
                "\n",
                "# Evaluate\n",
                "loss_reg, mae_reg = model_nn_reg.evaluate(X_test_reg, y_test_reg)\n",
                "print(f\"\\nNeural Network Test MAE: {mae_reg:.4f}\")\n",
                "\n",
                "# Predictions\n",
                "y_pred_nn_reg = model_nn_reg.predict(X_test_reg)\n",
                "print(f\"R2 Score (NN): {r2_score(y_test_reg, y_pred_nn_reg):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.4 Task 2: Classical Regression Models\n",
                "We will use **Linear Regression** and **Random Forest Regressor**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Linear Regression\n",
                "lin_reg = LinearRegression()\n",
                "lin_reg.fit(X_train_reg, y_train_reg)\n",
                "y_pred_lin = lin_reg.predict(X_test_reg)\n",
                "\n",
                "print(\"Linear Regression Performance:\")\n",
                "print(f\"MAE: {mean_absolute_error(y_test_reg, y_pred_lin):.4f}\")\n",
                "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_lin)):.4f}\")\n",
                "print(f\"R2 Score: {r2_score(y_test_reg, y_pred_lin):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Random Forest Regressor\n",
                "rf_reg = RandomForestRegressor(random_state=42, n_estimators=100)\n",
                "rf_reg.fit(X_train_reg, y_train_reg)\n",
                "y_pred_rf_reg = rf_reg.predict(X_test_reg)\n",
                "\n",
                "print(\"Random Forest Regressor Performance:\")\n",
                "print(f\"MAE: {mean_absolute_error(y_test_reg, y_pred_rf_reg):.4f}\")\n",
                "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_rf_reg)):.4f}\")\n",
                "print(f\"R2 Score: {r2_score(y_test_reg, y_pred_rf_reg):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.5 Hyperparameter Optimization\n",
                "Optimizing Random Forest Regressor."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameter Grid\n",
                "param_grid_reg = {\n",
                "    'n_estimators': [50, 100],\n",
                "    'max_depth': [None, 10, 20],\n",
                "    'min_samples_split': [2, 5]\n",
                "}\n",
                "\n",
                "# Randomized Search\n",
                "rf_random_reg = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=42), \n",
                "                                   param_distributions=param_grid_reg, \n",
                "                                   n_iter=5, \n",
                "                                   cv=3, \n",
                "                                   verbose=1, \n",
                "                                   random_state=42,\n",
                "                                   n_jobs=-1)\n",
                "\n",
                "rf_random_reg.fit(X_train_reg, y_train_reg)\n",
                "\n",
                "print(\"Best Regression Parameters:\", rf_random_reg.best_params_)\n",
                "best_rf_reg = rf_random_reg.best_estimator_\n",
                "\n",
                "# Evaluate\n",
                "y_pred_best_reg = best_rf_reg.predict(X_test_reg)\n",
                "print(f\"\\nOptimized Random Forest R2: {r2_score(y_test_reg, y_pred_best_reg):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.6 Feature Selection\n",
                "Selecting top features for Regression."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rfe_reg = RFE(estimator=RandomForestRegressor(random_state=42, n_estimators=50), n_features_to_select=8)\n",
                "rfe_reg.fit(X_train_reg, y_train_reg)\n",
                "\n",
                "selected_features_reg = pd.DataFrame({'Feature': X_reg.columns, 'Selected': rfe_reg.support_})\n",
                "print(\"Top 8 Selected Features:\")\n",
                "print(selected_features_reg[selected_features_reg['Selected'] == True])\n",
                "\n",
                "# Evaluate on Selected Features\n",
                "X_train_rfe_reg = rfe_reg.transform(X_train_reg)\n",
                "X_test_rfe_reg = rfe_reg.transform(X_test_reg)\n",
                "\n",
                "rf_rfe_reg = RandomForestRegressor(random_state=42, n_estimators=100)\n",
                "rf_rfe_reg.fit(X_train_rfe_reg, y_train_reg)\n",
                "y_pred_rfe_reg = rf_rfe_reg.predict(X_test_rfe_reg)\n",
                "\n",
                "print(f\"\\nRandom Forest with Selected Features R2: {r2_score(y_test_reg, y_pred_rfe_reg):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conclusion\n",
                "This notebook successfully implemented both classification and regression pipelines, including data loading, EDA, preprocessing, model building (both Neural Network and Classical), and optimization."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}