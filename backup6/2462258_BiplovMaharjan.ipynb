{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Final Portfolio Project 2026\n",
                "## 5CS037 - Concepts and Technologies of AI\n",
                "\n",
                "**Student Name:** Biplov Maharjan  \n",
                "**Student ID:** 2462258  \n",
                "\n",
                "---\n",
                "\n",
                "**Note:** This notebook implements Machine Learning algorithms **from scratch** using `numpy` to demonstrate a deep understanding of the underlying mathematical principles. `scikit-learn` is used primarily for data utilities (splitting, scaling) and performance metrics.\n",
                "\n",
                "## Table of Contents\n",
                "1. [Setup and Initialization](#setup)\n",
                "2. [Models from Scratch](#models)\n",
                "3. [Classification Task](#classification)\n",
                "4. [Regression Task](#regression)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Initialization <a id='setup'></a>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Minimal Sklearn usage for utilities\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "\n",
                "# Set plot style\n",
                "sns.set(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 8)\n",
                "\n",
                "print(\"Libraries imported successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Models from Scratch <a id='models'></a>\n",
                "\n",
                "In this section, I implemented the core logic for Neural Networks, Logistic Regression, Linear Regression, and KNN using NumPy matrix operations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NeuralNetwork:\n",
                "    def __init__(self, layers_structure, learning_rate=0.01, activation='relu', task='classification'):\n",
                "        self.layers_structure = layers_structure  # List of neurons per layer e.g., [input_dim, 64, 32, 1]\n",
                "        self.learning_rate = learning_rate\n",
                "        self.hidden_activation = activation\n",
                "        self.task = task  # 'classification' or 'regression'\n",
                "        self.params = {}\n",
                "        self.init_weights()\n",
                "        \n",
                "    def init_weights(self):\n",
                "        np.random.seed(42)\n",
                "        for i in range(1, len(self.layers_structure)):\n",
                "            self.params[f'W{i}'] = np.random.randn(self.layers_structure[i-1], self.layers_structure[i]) * 0.01\n",
                "            self.params[f'b{i}'] = np.zeros((1, self.layers_structure[i]))\n",
                "            \n",
                "    def sigmoid(self, Z):\n",
                "        return 1 / (1 + np.exp(-Z))\n",
                "    \n",
                "    def relu(self, Z):\n",
                "        return np.maximum(0, Z)\n",
                "    \n",
                "    def relu_deriv(self, Z):\n",
                "        return Z > 0\n",
                "    \n",
                "    def forward(self, X):\n",
                "        self.cache = {'A0': X}\n",
                "        L = len(self.layers_structure) - 1\n",
                "        \n",
                "        for i in range(1, L + 1):\n",
                "            W = self.params[f'W{i}']\n",
                "            b = self.params[f'b{i}']\n",
                "            A_prev = self.cache[f'A{i-1}']\n",
                "            \n",
                "            Z = np.dot(A_prev, W) + b\n",
                "            self.cache[f'Z{i}'] = Z\n",
                "            \n",
                "            if i == L:  # Output Layer\n",
                "                if self.task == 'classification':\n",
                "                    A = self.sigmoid(Z)\n",
                "                else:  # Regression (Linear)\n",
                "                    A = Z\n",
                "            else:  # Hidden Layer\n",
                "                A = self.relu(Z) if self.hidden_activation == 'relu' else self.sigmoid(Z)\n",
                "            \n",
                "            self.cache[f'A{i}'] = A\n",
                "            \n",
                "        return self.cache[f'A{L}']\n",
                "    \n",
                "    def backward(self, Y):\n",
                "        L = len(self.layers_structure) - 1\n",
                "        m = Y.shape[0]\n",
                "        gradients = {}\n",
                "        \n",
                "        # Initialize Backprop (Error at Output)\n",
                "        A_final = self.cache[f'A{L}']\n",
                "        \n",
                "        if self.task == 'classification':\n",
                "            dA = - (np.divide(Y, A_final + 1e-8) - np.divide(1 - Y, 1 - A_final + 1e-8))\n",
                "            dZ = A_final - Y # Simplification for Sigmoid + CrossEntropy\n",
                "        else:\n",
                "            dZ = 2 * (A_final - Y) / m  # MSE Derivative\n",
                "            \n",
                "        gradients[f'dZ{L}'] = dZ\n",
                "        gradients[f'dW{L}'] = np.dot(self.cache[f'A{L-1}'].T, dZ) / m\n",
                "        gradients[f'db{L}'] = np.sum(dZ, axis=0, keepdims=True) / m\n",
                "        \n",
                "        # Backprop through Hidden Layers\n",
                "        for i in range(L-1, 0, -1):\n",
                "            dZ_next = gradients[f'dZ{i+1}']\n",
                "            W_next = self.params[f'W{i+1}']\n",
                "            \n",
                "            dA = np.dot(dZ_next, W_next.T)\n",
                "            Z = self.cache[f'Z{i}']\n",
                "            \n",
                "            if self.hidden_activation == 'relu':\n",
                "                dZ = dA * self.relu_deriv(Z)\n",
                "            else:\n",
                "                s = self.sigmoid(Z)\n",
                "                dZ = dA * s * (1 - s)\n",
                "                \n",
                "            gradients[f'dZ{i}'] = dZ\n",
                "            gradients[f'dW{i}'] = np.dot(self.cache[f'A{i-1}'].T, dZ) / m\n",
                "            gradients[f'db{i}'] = np.sum(dZ, axis=0, keepdims=True) / m\n",
                "            \n",
                "        return gradients\n",
                "    \n",
                "    def update_params(self, gradients):\n",
                "        L = len(self.layers_structure) - 1\n",
                "        for i in range(1, L + 1):\n",
                "            self.params[f'W{i}'] -= self.learning_rate * gradients[f'dW{i}']\n",
                "            self.params[f'b{i}'] -= self.learning_rate * gradients[f'db{i}']\n",
                "\n",
                "    def fit(self, X, Y, epochs=1000, verbose=True):\n",
                "        if Y.ndim == 1:\n",
                "            Y = Y.reshape(-1, 1)\n",
                "            \n",
                "        loss_history = []\n",
                "        \n",
                "        for i in range(epochs):\n",
                "            # Forward\n",
                "            Y_hat = self.forward(X)\n",
                "            \n",
                "            # Loss Calculation\n",
                "            if self.task == 'classification':\n",
                "                loss = -np.mean(Y * np.log(Y_hat + 1e-8) + (1 - Y) * np.log(1 - Y_hat + 1e-8))\n",
                "            else:\n",
                "                loss = np.mean(np.square(Y - Y_hat))\n",
                "            \n",
                "            loss_history.append(loss)\n",
                "            \n",
                "            # Backward & Update\n",
                "            grads = self.backward(Y)\n",
                "            self.update_params(grads)\n",
                "            \n",
                "            if verbose and i % (epochs // 10) == 0:\n",
                "                print(f\"Epoch {i}: Loss {loss:.4f}\")\n",
                "                \n",
                "        return loss_history\n",
                "    \n",
                "    def predict(self, X):\n",
                "        Y_hat = self.forward(X)\n",
                "        if self.task == 'classification':\n",
                "            return (Y_hat > 0.5).astype(int)\n",
                "        return Y_hat"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LogisticRegressionScratch:\n",
                "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
                "        self.lr = learning_rate\n",
                "        self.iterations = iterations\n",
                "        self.weights = None\n",
                "        self.bias = None\n",
                "        \n",
                "    def sigmoid(self, z):\n",
                "        return 1 / (1 + np.exp(-z))\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        n_samples, n_features = X.shape\n",
                "        self.weights = np.zeros(n_features)\n",
                "        self.bias = 0\n",
                "\n",
                "        # Gradient Descent\n",
                "        for _ in range(self.iterations):\n",
                "            model = np.dot(X, self.weights) + self.bias\n",
                "            y_predicted = self.sigmoid(model)\n",
                "            \n",
                "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
                "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
                "            \n",
                "            self.weights -= self.lr * dw\n",
                "            self.bias -= self.lr * db\n",
                "\n",
                "    def predict(self, X):\n",
                "        linear_model = np.dot(X, self.weights) + self.bias\n",
                "        y_predicted = self.sigmoid(linear_model)\n",
                "        return [1 if i > 0.5 else 0 for i in y_predicted]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LinearRegressionScratch:\n",
                "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
                "        self.lr = learning_rate\n",
                "        self.iterations = iterations\n",
                "        self.weights = None\n",
                "        self.bias = None\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        n_samples, n_features = X.shape\n",
                "        self.weights = np.zeros(n_features)\n",
                "        self.bias = 0\n",
                "        \n",
                "        for _ in range(self.iterations):\n",
                "            y_predicted = np.dot(X, self.weights) + self.bias\n",
                "            \n",
                "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
                "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
                "            \n",
                "            self.weights -= self.lr * dw\n",
                "            self.bias -= self.lr * db\n",
                "\n",
                "    def predict(self, X):\n",
                "        return np.dot(X, self.weights) + self.bias"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class KNNScratch:\n",
                "    def __init__(self, k=3, task='classification'):\n",
                "        self.k = k\n",
                "        self.task = task\n",
                "\n",
                "    def fit(self, X, y):\n",
                "        self.X_train = np.array(X)\n",
                "        self.y_train = np.array(y)\n",
                "\n",
                "    def predict(self, X):\n",
                "        y_pred = [self._predict(x) for x in np.array(X)]\n",
                "        return np.array(y_pred)\n",
                "\n",
                "    def _predict(self, x):\n",
                "        # Compute distances\n",
                "        distances = [np.sqrt(np.sum((x_train - x) ** 2)) for x_train in self.X_train]\n",
                "        # Get k nearest samples and their labels\n",
                "        k_indices = np.argsort(distances)[:self.k]\n",
                "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
                "        \n",
                "        if self.task == 'classification':\n",
                "            # Majority Vote\n",
                "            most_common = max(set(k_nearest_labels), key=k_nearest_labels.count)\n",
                "            return most_common\n",
                "        else:\n",
                "            # Mean\n",
                "            return np.mean(k_nearest_labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Classification Task <a id='classification'></a>\n",
                "\n",
                "We use the **Depression Student Dataset** for binary classification.\n",
                "Target: `Depression` (0 or 1)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Data\n",
                "class_data_path = r'classification/student_depression_dataset.csv'\n",
                "try:\n",
                "    df_class = pd.read_csv(class_data_path)\n",
                "    # Preprocessing\n",
                "    df_class = df_class.drop(columns=['id', 'City'], errors='ignore')\n",
                "    \n",
                "    # Encoding\n",
                "    le = LabelEncoder()\n",
                "    cat_cols = df_class.select_dtypes(include=['object']).columns\n",
                "    for col in cat_cols:\n",
                "        df_class[col] = le.fit_transform(df_class[col].astype(str))\n",
                "    \n",
                "    X = df_class.drop('Depression', axis=1).values # Convert to simple numpy array\n",
                "    y = df_class['Depression'].values\n",
                "    \n",
                "    # Split and Scale\n",
                "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "    scaler = StandardScaler()\n",
                "    X_train = scaler.fit_transform(X_train)\n",
                "    X_test = scaler.transform(X_test)\n",
                "    \n",
                "    print(\"Classification Data Ready:\", X_train.shape)\n",
                "except Exception as e:\n",
                "    print(\"Error loading data:\", e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Train Neural Network (Scratch)\n",
                "input_dim = X_train.shape[1]\n",
                "nn_class = NeuralNetwork(layers_structure=[input_dim, 64, 32, 1], learning_rate=0.01, task='classification')\n",
                "\n",
                "print(\"Training Custom Neural Network...\")\n",
                "loss_hist = nn_class.fit(X_train, y_train, epochs=2000, verbose=False)\n",
                "\n",
                "plt.plot(loss_hist)\n",
                "plt.title('NN Training Loss')\n",
                "plt.show()\n",
                "\n",
                "y_pred_nn = nn_class.predict(X_test)\n",
                "print(\"Neural Network Evaluation:\")\n",
                "print(classification_report(y_test, y_pred_nn))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Train Logistic Regression (Scratch)\n",
                "log_reg = LogisticRegressionScratch(learning_rate=0.01, iterations=2000)\n",
                "print(\"Training Custom Logistic Regression...\")\n",
                "log_reg.fit(X_train, y_train)\n",
                "\n",
                "y_pred_log = log_reg.predict(X_test)\n",
                "print(\"Logistic Regression Evaluation:\")\n",
                "print(classification_report(y_test, y_pred_log))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Train KNN Classifier (Scratch)\n",
                "# Using distinct subset to speed up KNN as it is computationally expensive\n",
                "X_train_sub, _, y_train_sub, _ = train_test_split(X_train, y_train, train_size=2000, random_state=42)\n",
                "\n",
                "knn = KNNScratch(k=5, task='classification')\n",
                "print(\"Training Custom KNN...\")\n",
                "knn.fit(X_train_sub, y_train_sub)\n",
                "\n",
                "y_pred_knn = knn.predict(X_test[:1000]) # Eval on subset for speed\n",
                "print(\"KNN Evaluation (on subset):\")\n",
                "print(classification_report(y_test[:1000], y_pred_knn))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Regression Task <a id='regression'></a>\n",
                "\n",
                "We use the **Avocado Dataset** for regression.\n",
                "Target: `AveragePrice`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Data\n",
                "reg_data_path = r'regression/avocado.csv'\n",
                "try:\n",
                "    df_reg = pd.read_csv(reg_data_path)\n",
                "    df_reg = df_reg.drop(columns=['Unnamed: 0', 'Date'], errors='ignore')\n",
                "    \n",
                "    le = LabelEncoder()\n",
                "    for col in ['type', 'region']:\n",
                "        df_reg[col] = le.fit_transform(df_reg[col])\n",
                "        \n",
                "    X_reg = df_reg.drop('AveragePrice', axis=1).values\n",
                "    y_reg = df_reg['AveragePrice'].values\n",
                "    \n",
                "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
                "    \n",
                "    scaler_reg = StandardScaler()\n",
                "    X_train_reg = scaler_reg.fit_transform(X_train_reg)\n",
                "    X_test_reg = scaler_reg.transform(X_test_reg)\n",
                "    \n",
                "    print(\"Regression Data Ready:\", X_train_reg.shape)\n",
                "except Exception as e:\n",
                "    print(\"Error loading data:\", e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Train Neural Network Regressor (Scratch)\n",
                "input_dim_reg = X_train_reg.shape[1]\n",
                "nn_reg = NeuralNetwork(layers_structure=[input_dim_reg, 64, 32, 1], learning_rate=0.001, task='regression')\n",
                "\n",
                "print(\"Training Custom Neural Network Regressor...\")\n",
                "loss_hist_reg = nn_reg.fit(X_train_reg, y_train_reg, epochs=1000, verbose=False)\n",
                "\n",
                "plt.plot(loss_hist_reg)\n",
                "plt.title('Regression Training Loss (MSE)')\n",
                "plt.show()\n",
                "\n",
                "y_pred_nn_reg = nn_reg.predict(X_test_reg).flatten()\n",
                "print(f\"NN R2 Score: {r2_score(y_test_reg, y_pred_nn_reg):.4f}\")\n",
                "print(f\"NN MAE: {mean_absolute_error(y_test_reg, y_pred_nn_reg):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Train Linear Regression (Scratch)\n",
                "lin_reg = LinearRegressionScratch(learning_rate=0.01, iterations=2000)\n",
                "print(\"Training Custom Linear Regression...\")\n",
                "lin_reg.fit(X_train_reg, y_train_reg)\n",
                "\n",
                "y_pred_lin = lin_reg.predict(X_test_reg)\n",
                "print(f\"Linear Regression R2 Score: {r2_score(y_test_reg, y_pred_lin):.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}